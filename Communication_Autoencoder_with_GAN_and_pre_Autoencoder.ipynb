{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Communication Autoencoder with GAN and pre-Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenYavor/MA_GAN/blob/master/Communication_Autoencoder_with_GAN_and_pre_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3Mvn1V30ejH",
        "colab_type": "code",
        "outputId": "eac22e74-7e3a-4d00-95e5-01a19fec05f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt   \n",
        "import warnings\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
        "    import tensorflow as tf\n",
        "import os\n",
        "tf.__version__\n",
        "from tensorflow import keras\n",
        "import time\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import pandas as pd\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "from scipy import special\n",
        "#from Clustering_Equalgrps.equal_groups import EqualGroupsKMeans\n",
        "from tensorflow.keras import layers\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 40kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.10.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 36.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.17.4)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 31.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (42.0.2)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/31/f944cbd5bdbcc90d5b36f0615036308c8ec1e41b4788da5b55d4900f6803/google_auth-1.8.2-py2.py3-none-any.whl (75kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.21.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.7)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.11.28)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
            "\u001b[31mERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.8.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, google-auth, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed google-auth-1.8.2 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wlZswcMF7Rt",
        "colab_type": "text"
      },
      "source": [
        "#### Vergleich\n",
        "Als erstes für feste $k$ und $n$, was sich ändert ist die Samplesize, Anzahl der Samples und SNR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qpY-gawAf-9",
        "colab_type": "text"
      },
      "source": [
        "###Systemparameter\n",
        "ACHTUNG: CHANNELANZAHL WURDE UNTERSCHIEDLICH VERWENDET \\\\\n",
        "$k$ - die Anzhal der bits \\\\\n",
        "$M$ - Anzahl der unterschiedlichen Nachrichten \\\\\n",
        "$n$ - channel uses\\\\\n",
        "$N$ - Länge des Rauschvektors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "czeNNfpY1qc2",
        "outputId": "54e92c35-f8d4-4e73-bd2a-6002f2126547",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "k = 4      # Number of information bits per message, i.e., M=2**k\n",
        "M = 2**k\n",
        "n = 2    # Number of real channel uses per message\n",
        "#k = int(np.log2(M))\n",
        "#n = 2\n",
        "print(M)\n",
        "\n",
        "SNR = 7\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA4TqJBOXXIg",
        "colab_type": "text"
      },
      "source": [
        "## Training Parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJZ_cnp9V-Q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_learning_rate=0.0001 \n",
        "disc_learning_rate = 0.0005                  # 0.0001  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb-DiBwSN255",
        "colab_type": "text"
      },
      "source": [
        "### Different Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFMMLrY0LthL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randN_initial = keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)\n",
        "\n",
        "EncIn = tf.keras.layers.Input(shape=(M,))#, dtype= tf.int32)\n",
        "e1 = tf.keras.layers.Dense(2*n, activation=None)\n",
        "e2 = tf.keras.layers.Lambda(lambda x:tf.reshape(x, shape=[-1,int(n/2),2]))\n",
        "EncOut = tf.keras.layers.Lambda(lambda x: x/tf.sqrt(2*tf.reduce_mean(tf.square(x))))\n",
        "GenIn = tf.keras.layers.Lambda(lambda x:tf.reshape(x,(tf.shape(x)[0],-1)))\n",
        "# = tf.keras.layers.Lambda(generator)\n",
        "DecIn = tf.keras.layers.Lambda(lambda x:tf.reshape(x, shape=[-1,int(n/2),2]))\n",
        "d1 = tf.keras.layers.Lambda(lambda x:tf.reshape(x, shape=[-1,n]))\n",
        "d2 = tf.keras.layers.Dense(M, activation='relu')\n",
        "DecOut = tf.keras.layers.Dense(M, activation='softmax')\n",
        "\n",
        "\n",
        "#noise_std = EbNo_to_noise(TRAINING_SNR)\n",
        "# custom functions / layers without weights\n",
        "norm_layer = keras.layers.Lambda(lambda x: tf.divide(x,tf.sqrt(2*tf.reduce_mean(tf.square(x)))))\n",
        "shape_layer = keras.layers.Lambda(lambda x: tf.reshape(x, shape=[-1,2,n]))\n",
        "shape_layer2 = keras.layers.Lambda(lambda x: tf.reshape(x, shape=[-1,n]))\n",
        "channel_layer = keras.layers.Lambda(lambda x: x + tf.random.normal(tf.shape(x), mean=0.0, stddev=noise_std))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7J96hJhKO9VJ",
        "colab_type": "text"
      },
      "source": [
        "### Help functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV7pjryDv4M4",
        "colab_type": "code",
        "outputId": "30b4f12b-b273-4433-b2e6-c24e8418e79d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "def EbNo2Sigma(ebnodb):\n",
        "    '''Convert Eb/No in dB to noise standard deviation'''\n",
        "    ebno = 10**(ebnodb/10)\n",
        "    return 1/np.sqrt(2*(2*k/n)*ebno)\n",
        "\n",
        "def EbNo_to_noise(ebnodb):\n",
        "    '''Transform EbNo[dB]/snr to noise power'''\n",
        "    ebno = 10**(ebnodb/10)\n",
        "    noise_std = 1/np.sqrt(2*(2*k/n)*ebno) \n",
        "    return noise_std\n",
        "\n",
        "\n",
        "def real_channel(x,noise_std):\n",
        "    # Black-box Channel\n",
        "    #AWGN\n",
        "    return x + tf.random.normal(tf.shape(x), mean=0.0, stddev=noise_std)\n",
        "\n",
        "    #Rayleigh\n",
        "    #return x + tf.sqrt(tf.square(tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std)) + tf.square(tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std)))\n",
        "    \n",
        "    #Uniform U(-3;3)    \n",
        "    #return x + tf.random_uniform(tf.shape(x), minval=-2, maxval=2)\n",
        "\n",
        "def B_Ber(input_msg, msg):\n",
        "    '''Calculate the Batch Bit Error Rate'''\n",
        "    pred_error = tf.not_equal(tf.argmax(msg, 1), tf.argmax(input_msg, 1))\n",
        "    bber = tf.reduce_mean(tf.cast(pred_error, tf.float32))\n",
        "    return bber\n",
        "\n",
        "def random_sample(batch_size=32):\n",
        "    msg = np.random.randint(M, size=batch_size)\n",
        "    return msg\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def B_Ber_m(input_msg, msg):\n",
        "    '''Calculate the Batch Bit Error Rate'''\n",
        "    pred_error = tf.not_equal(input_msg, tf.argmax(msg, 1))      \n",
        "    bber = tf.reduce_mean(tf.cast(pred_error, tf.float32))\n",
        "    return bber\n",
        "\n",
        "def SNR_to_noise(snrdb):\n",
        "    '''Transform EbNo[dB]/snr to noise power'''\n",
        "    snr = 10**(snrdb/10)\n",
        "    noise_std = 1/np.sqrt(2*snr)\n",
        "    return noise_std\n",
        "\n",
        "noise_std = EbNo2Sigma(SNR)\n",
        "\n",
        "print(EbNo2Sigma(SNR))\n",
        "print(EbNo_to_noise(SNR))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.15792649852735607\n",
            "0.15792649852735607\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBGMgdDEh7uX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_encoding(M=16, n=1):\n",
        "    inp = np.arange(0,M)\n",
        "    coding = gan_encoder.predict(inp)\n",
        "    fig = plt.figure(figsize=(4,4))\n",
        "    plt.plot(coding[:,0], coding[:, 1], \"b.\")\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
        "    plt.grid(True)\n",
        "    plt.gca().set_ylim(-2, 2)\n",
        "    plt.gca().set_xlim(-2, 2)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def test_encoding_pre(M=16, n=1):\n",
        "    inp = np.arange(0,M)\n",
        "    coding = pre_encoder.predict(inp)\n",
        "    fig = plt.figure(figsize=(4,4))\n",
        "    plt.plot(coding[:,0], coding[:, 1], \"b.\")\n",
        "    plt.xlabel(\"$x_1$\", fontsize=18)\n",
        "    plt.ylabel(\"$x_2$\", fontsize=18, rotation=0)\n",
        "    plt.grid(True)\n",
        "    plt.gca().set_ylim(-2, 2)\n",
        "    plt.gca().set_xlim(-2, 2)\n",
        "    plt.show()    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOoYuK_jR9rH",
        "colab_type": "text"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQxhmgOa0_7c",
        "colab_type": "text"
      },
      "source": [
        "#### Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXbS5lM9Tb9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_generator(n):\n",
        "  input1 = tf.keras.layers.Input(shape=(n,))\n",
        "  x1 = tf.keras.layers.Dense(n)(input1)\n",
        "  input2 =tf.random.normal(shape=tf.shape(input1))\n",
        "  #input2 =tf.random.normal([tf.shape(input1)[0],n])\n",
        "  x2 = tf.keras.layers.Dense(n)(input2)\n",
        "  subtracted = tf.keras.layers.Concatenate(1)([x1, x2])\n",
        "  h1 = tf.keras.layers.Dense(64,use_bias=True,  activation='relu')(subtracted)\n",
        "  h2 = tf.keras.layers.Dense(64,use_bias=True, activation='relu')(h1)\n",
        "  out = tf.keras.layers.Dense(n, use_bias= True, activation='linear')(h2)\n",
        "  generator = tf.keras.models.Model(inputs=[input1], outputs=out)\n",
        "  return generator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt2rTP7hSFt4",
        "colab_type": "text"
      },
      "source": [
        "#### Discriminator Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97h2eMLeXS68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_discriminator(n):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(32,use_bias=True, kernel_initializer=randN_initial,activation='relu',input_shape=((2*n,))))\n",
        "  model.add(tf.keras.layers.Dense(32,use_bias=True, kernel_initializer=randN_initial, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(1,use_bias=False, activation='sigmoid'))\n",
        "  return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rIdAYRhHgGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = get_generator(n)\n",
        "discriminator = get_discriminator(n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcIzLZj5Seh9",
        "colab_type": "text"
      },
      "source": [
        "## Encoder and Pre-Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNHtzAC4SPBq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_gan_encoder(M):\n",
        "  model = keras.models.Sequential([\n",
        "            keras.layers.Embedding(M, M, embeddings_initializer='glorot_normal'),\n",
        "            keras.layers.Dense(M*2, activation=\"elu\"),\n",
        "            keras.layers.Dense(M*2, activation=\"elu\"),\n",
        "            keras.layers.Dense(n, activation=None),\n",
        "            e2,\n",
        "            EncOut,\n",
        "            GenIn])\n",
        "  return model\n",
        "\n",
        "def get_pre_encoder(M):\n",
        "  model = keras.models.Sequential([\n",
        "            keras.layers.Embedding(M, M, embeddings_initializer='glorot_normal'),\n",
        "            keras.layers.Dense(M,kernel_initializer=randN_initial, activation=\"elu\"),\n",
        "            keras.layers.Dense(n,kernel_initializer=randN_initial, activation=None),\n",
        "            e2,\n",
        "            EncOut,\n",
        "            GenIn])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5iCDE4dSL35",
        "colab_type": "text"
      },
      "source": [
        "## Decoder and Pre-Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5KjEhDvSWQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_gan_decoder(M):\n",
        "   model= keras.models.Sequential([\n",
        "                #DecIn,\n",
        "                #d1,\n",
        "                keras.layers.Input(shape=(n,)),\n",
        "                keras.layers.Dense(M*2, activation=\"elu\"),\n",
        "                keras.layers.Dense(M*2, activation=\"elu\"),\n",
        "                keras.layers.Dense(M, activation=\"softmax\")\n",
        "                ])\n",
        "   return model\n",
        "\n",
        "\n",
        "def get_pre_decoder(M):\n",
        "   model= keras.models.Sequential([\n",
        "                #DecIn,\n",
        "                #d1,\n",
        "                keras.layers.Input(shape=(n,)),\n",
        "                keras.layers.Dense(M,kernel_initializer=randN_initial, activation=\"elu\"),\n",
        "                keras.layers.Dense(M,kernel_initializer=randN_initial, activation=\"softmax\")\n",
        "                ])\n",
        "   return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-t76sYBtlP1",
        "colab_type": "text"
      },
      "source": [
        "### GAN Training functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n15AvPFO05gd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gan_optimizers(gen_learning_rate  ,disc_learning_rate):\n",
        "  generator_optimizer = tf.keras.optimizers.RMSprop(gen_learning_rate)      #RMSprop   in oreder to test where the error comes from\n",
        "  discriminator_optimizer = tf.keras.optimizers.RMSprop(disc_learning_rate) \n",
        "  return generator_optimizer, discriminator_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooDukkHvmduJ",
        "colab_type": "code",
        "outputId": "42f92ab4-7037-4396-b676-9fe4764c2d93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "%%time\n",
        "def train_gan(epochs,n_steps, batch_size, SNR_level):\n",
        "  noise_std = EbNo2Sigma(SNR_level)\n",
        "  start = time.time()\n",
        "  x = tf.random.normal((batch_size,n),dtype=tf.dtypes.float32) \n",
        "  x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x)))\n",
        "  counter = 0\n",
        "  epoch = 0\n",
        "  for epoch in range(epochs):\n",
        "    counter += 1\n",
        "    train_step(noise_std, n_steps,batch_size)\n",
        "    if counter%100==0:\n",
        "      tf.print(\"counter %d:\" % (counter))\n",
        "      fake_c = generator(x)\n",
        "      tf.print(fake_c[0])\n",
        "    #print ('Time for epoch {} is {} sec,'.format(epoch + 1, time.time()-start))\n",
        "      tf.print ('Time for epoch {},'.format(epoch + 1))\n",
        "      \n",
        "  tf.saved_model.save(generator,'/tmp/saved_model/')\n",
        "  tf.print ('Time for the training is {} sec,'.format( time.time()-start))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
            "Wall time: 7.63 µs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WE_JS7kgA1W-",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(noise_std,n_steps,batch_size):\n",
        "  x = tf.random.normal((batch_size,n),dtype=tf.dtypes.float32) \n",
        "  x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x)))\n",
        "  m =random_sample(batch_size)\n",
        "  r = pre_encoder(m)\n",
        "  for i in range(n_steps):\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      real_training_data = tf.concat(values=[real_channel(r,noise_std), r], axis=1)\n",
        "      fake_training_data = tf.concat(values=[generator(r),r], axis=1)\n",
        "      real_output = discriminator(real_training_data)\n",
        "      fake_output = discriminator(fake_training_data)\n",
        "      \n",
        "      \n",
        "      disc_loss = -tf.reduce_mean(tf.math.log(real_output) + tf.math.log(1. - fake_output))\n",
        "      gen_loss =-tf.reduce_mean(tf.math.log(fake_output))\n",
        "      \n",
        "      #tf.print(disc_loss,gen_loss)\n",
        "      \n",
        "      if tf.math.is_nan(disc_loss) == False:\n",
        "        gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "        discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    \n",
        "      if i == 4:  \n",
        "        gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "        generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y82FQj3Jmvxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def creating_and_train_gan(epochs,n_steps, batch_size, SNR_level , n ):  #optional Leraning Rates\n",
        "  train_gan(epochs, n_steps, batch_size, SNR_level)\n",
        "  #4 after GAN training\n",
        "  generator.trainable = False\n",
        "  tf.print(generator.trainable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLTtFoV0IoPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gan_Test_AE(data):\n",
        "    '''Calculate Bit Error for varying SNRs'''\n",
        "    snr_range = np.linspace(0, 15, 31)\n",
        "    bber_vec = [None] * len(snr_range)\n",
        "        \n",
        "    for db in range(len(snr_range)):           \n",
        "        noise_std = EbNo_to_noise(snr_range[db])\n",
        "        code_word = gan_encoder(data)\n",
        "        rcvd_word = code_word + tf.random.normal(tf.shape(code_word), mean=0.0, stddev=noise_std)\n",
        "        dcoded_msg = gan_decoder(rcvd_word)\n",
        "        bber_vec[db] = B_Ber_m(data, dcoded_msg)\n",
        "        if (db % 6 == 0) & (db > 0):\n",
        "            print(f'Progress: {db} of {30} parts')\n",
        "\n",
        "    return (snr_range, bber_vec)\n",
        "\n",
        "def w_gan_Test_AE(data):\n",
        "    '''Calculate Bit Error for varying SNRs'''\n",
        "    snr_range = np.linspace(0, 15, 31)\n",
        "    bber_vec = [None] * len(snr_range)\n",
        "        \n",
        "    for db in range(len(snr_range)):           \n",
        "        noise_std = EbNo_to_noise(snr_range[db])\n",
        "        code_word = w_gan_encoder(data)\n",
        "        rcvd_word = code_word + tf.random.normal(tf.shape(code_word), mean=0.0, stddev=noise_std)\n",
        "        dcoded_msg = w_gan_decoder(rcvd_word)\n",
        "        bber_vec[db] = B_Ber_m(data, dcoded_msg)\n",
        "        if (db % 6 == 0) & (db > 0):\n",
        "            print(f'Progress: {db} of {30} parts')\n",
        "\n",
        "    return (snr_range, bber_vec)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI0Tj40kzY6X",
        "colab_type": "text"
      },
      "source": [
        "# Pre Autoencoder Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS0X_xB4LBXa",
        "colab_type": "code",
        "outputId": "9224fb6d-6820-426b-e5b0-b5ba2de7981a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        }
      },
      "source": [
        "pre_decoder = get_pre_decoder(M)\n",
        "pre_encoder = get_pre_encoder(M)\n",
        "\n",
        "\n",
        "\n",
        "gan_AE = tf.keras.models.Sequential([pre_encoder,pre_decoder])\n",
        "data, test_data = random_sample(10000000), random_sample(10000)\n",
        "start = time.time()\n",
        "gan_AE.compile(optimizer=keras.optimizers.Nadam(lr=0.005),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "history = gan_AE.fit(data, data, batch_size=500,steps_per_epoch=400, epochs=5)\n",
        "#pre_encoder.trainable = False\n",
        "#pre_decoder.trainable = False\n",
        "gan_AE.summary() \n",
        "test_encoding_pre(M,n) \n",
        "\n",
        "print(data.shape)\n",
        "print(pre_encoder(data).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 10000000 samples\n",
            "Epoch 1/5\n",
            "  198500/10000000 [..............................] - ETA: 2:37 - loss: 0.8276 - accuracy: 0.8031Epoch 2/5\n",
            "  199500/10000000 [..............................] - ETA: 1:00 - loss: 0.0341 - accuracy: 1.0000Epoch 3/5\n",
            "  196000/10000000 [..............................] - ETA: 56s - loss: 0.0093 - accuracy: 1.0000Epoch 4/5\n",
            "  198500/10000000 [..............................] - ETA: 58s - loss: 0.0043 - accuracy: 1.0000Epoch 5/5\n",
            "  200000/10000000 [..............................] - ETA: 1:03 - loss: 0.0024 - accuracy: 1.0000Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential_5 (Sequential)    (None, None)              562       \n",
            "_________________________________________________________________\n",
            "sequential_4 (Sequential)    (None, 16)                320       \n",
            "=================================================================\n",
            "Total params: 882\n",
            "Trainable params: 882\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASMAAAEWCAYAAAAtl/EzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(10000000,)\n",
            "(10000000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3m0fQ6OXgPf1",
        "colab_type": "text"
      },
      "source": [
        "# GAN Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNFwwZrcgOkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "generator_optimizer, discriminator_optimizer = gan_optimizers(gen_learning_rate=gen_learning_rate, disc_learning_rate = disc_learning_rate)\n",
        "start = time.time()\n",
        "creating_and_train_gan(epochs= 5000,n_steps=5, batch_size =100, SNR_level = 7, n = n)\n",
        "time_to_train_gan = time.time()-start\n",
        "tf.print ('Time for the training is {} sec,'.format( time.time()-start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GW2opX7SwMo",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# AE training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiuN3SZYpeTU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "gan_decoder = get_gan_decoder(M)\n",
        "gan_encoder = get_gan_encoder(M)\n",
        "\n",
        "\n",
        "\n",
        "gan_AE = tf.keras.models.Sequential([gan_encoder,generator,gan_decoder])\n",
        "data, test_data = random_sample(10000000), random_sample(10000)\n",
        "start = time.time()\n",
        "gan_AE.compile(optimizer=keras.optimizers.Nadam(lr=0.001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "history = gan_AE.fit(data, data, batch_size=500,steps_per_epoch=400, epochs=10)\n",
        "time_to_train_gan += time.time()-start\n",
        "tf.print ('Time for the training is {} sec,'.format( time.time()-start))\n",
        "gan_AE.summary()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngrucnfWBOHl",
        "colab_type": "text"
      },
      "source": [
        "### Training MI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5B2TUanPC5d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan_encoder.trainable = False\n",
        "gan_decoder.trainable = False\n",
        "\n",
        "test_encoding(M,n)   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukO76l6yIoPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test msg sequence for normal encoding\n",
        "N_test = 500000\n",
        "test_msg = np.random.randint(M, size=N_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK5wA-zzHScv",
        "colab_type": "text"
      },
      "source": [
        "### Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7M-S0sbhIoPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gan_bber_data = gan_Test_AE(test_msg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYdEm0eQIoP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Approximate 16 QAM Error\n",
        "def SIXT_QAM_sim(ebno):\n",
        "    return (3.0/2)*special.erfc(np.sqrt((4.0/10)*10.**(ebno/10)))\n",
        "\n",
        "ebnodbs = np.linspace(0,15,16)\n",
        "fig = plt.figure(figsize=(8, 5))\n",
        "plt.semilogy(gan_bber_data[0], gan_bber_data[1], '^-')\n",
        "plt.semilogy(ebnodbs, SIXT_QAM_sim(ebnodbs), '*-');\n",
        "plt.gca().set_ylim(1e-5, 1)\n",
        "plt.gca().set_xlim(0, 15)\n",
        "plt.ylabel(\"Batch Symbol Error Rate\", fontsize=14, rotation=90)\n",
        "plt.xlabel(\"SNR [dB]\", fontsize=18)\n",
        "plt.legend(['AE with GAN', '16QAM'],\n",
        "           prop={'size': 14}, loc='upper right');\n",
        "plt.grid(True, which=\"both\")\n",
        "print('time to train the AE Model with GAN',time_to_train_gan)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBes21qLlcS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generator.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q60pg9b9Rz7C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(gan_bber_data[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJl0KB-5at3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}