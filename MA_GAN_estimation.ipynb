{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MA_GAN_estimation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BenYavor/MA_GAN/blob/master/MA_GAN_estimation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-49-RQG7bEV",
        "colab_type": "code",
        "outputId": "0cacd915-8cee-4719-8909-70b01d5e9bc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0-rc0\n",
        "!pip install -q pyyaml h5py\n",
        "#!pip install -q tf_nightly\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt   \n",
        "import warnings\n",
        "with warnings.catch_warnings():\n",
        "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
        "    import tensorflow as tf\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\" \n",
        "tf.__version__\n",
        "from tensorflow import keras\n",
        "import time"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fb/4b/77f0965ec7e8a76d3dcd6a22ca8bbd2b934cd92c4ded43fef6bea5ff3258/tensorflow-2.0.0rc0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 46.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.11.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Collecting tb-nightly<1.15.0a20190807,>=1.15.0a20190806 (from tensorflow==2.0.0-rc0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/88/24b5fb7280e74c7cf65bde47c171547fd02afb3840cff41bcbe9270650f5/tb_nightly-1.15.0a20190806-py3-none-any.whl (4.3MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3MB 33.9MB/s \n",
            "\u001b[?25hCollecting tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601 (from tensorflow==2.0.0-rc0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/21/28/f2a27a62943d5f041e4a6fd404b2d21cb7c59b2242a4e73b03d9ba166552/tf_estimator_nightly-1.14.0.dev2019080601-py2.py3-none-any.whl (501kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.16.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.8.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.2.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (3.7.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (0.33.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.15.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-rc0) (1.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (0.15.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow==2.0.0-rc0) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0-rc0) (2.8.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed tb-nightly-1.15.0a20190806 tensorflow-2.0.0rc0 tf-estimator-nightly-1.14.0.dev2019080601\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qa9PJHQS0UCJ",
        "colab_type": "text"
      },
      "source": [
        "## System funktionsweise Allgemeine Daten\n",
        "\n",
        "#### Rauschen\n",
        "genarats-> **shape**: batch_size * number_of_real_channels_uses_per_message \\\\\n",
        "and does a average power normalization\n",
        "\n",
        "\n",
        "#### Generator\n",
        "Eingang: (2*n,32)  ; Ausgang: (32,n)   \\\\\n",
        "Loss-Function:\n",
        "\n",
        "#### Discriminator\n",
        "Eingang: (2*n,32)  ; Ausgang: (32,1)  \\\\\n",
        "Loss-Function:\n",
        "\n",
        "\n",
        "#### Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qpY-gawAf-9",
        "colab_type": "text"
      },
      "source": [
        "###Systemparameter\n",
        "$k$ - die Anzhal der bits \\\\\n",
        "$M$ - Anzahl der unterschiedlichen Nachrichten \\\\\n",
        "$n$ - channel uses **What is meant by that??** \\\\\n",
        "$N$ - Länge des Rauschvektors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86Y2r6qBAgKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = 4       # Number of information bits per message, i.e., M=2**k\n",
        "M = 2**k\n",
        "n = 8       # Number of real channel uses per message\n",
        "seed = 2    # Seed RNG reproduce identical results\n",
        "D_nb_weights = 32\n",
        "G_nb_weights = 32\n",
        "\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "\n",
        "x = tf.random.normal((batch_size,n))    #randomly sample input data (\"fake\" AE messages)\n",
        "x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) #Average power normalization (not required if standard normal distribution is used )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY9sHsfWT8By",
        "colab_type": "text"
      },
      "source": [
        "## Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXbS5lM9Tb9B",
        "colab_type": "code",
        "outputId": "d628c2c5-0978-4965-cc97-41b82c7df654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "\n",
        "\n",
        "#def generator(x):\n",
        "    # Concatenate z and y\n",
        "#    G_n = tf.random.normal([tf.shape(x)[0],n],dtype=tf.float32)  #create noise directly within the generator  \n",
        "#    inputs = tf.concat(values=[x, G_n], axis=1)\n",
        "    #dense NN\n",
        "#    G_h1 = tf.nn.relu(tf.matmul(inputs, G_W1) + G_b1)\n",
        "#    G_h2 = tf.nn.relu(tf.matmul(G_h1, G_W2) + G_b2)\n",
        "#    G_lin = tf.matmul(G_h2, G_W3) + G_b3\n",
        "    #G_prob = tf.nn.sigmoid(G_lin)\n",
        "#    return G_lin\n",
        "\n",
        "def generator_noise(input):\n",
        "  G_n = tf.random.normal([tf.shape(input)[0],n],dtype=tf.float32)  #create noise directly within the generator  \n",
        "  inputs = tf.concat(values=[input, G_n], axis=1)\n",
        "  return inputs\n",
        "    \n",
        "def generator(x = tf.keras.Input(shape=(batch_size,)),training = False):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Lambda(generator_noise))\n",
        "  model.add(tf.keras.layers.Dense(32,use_bias=True, activation='relu'))#, input_shape=(2*n,))\n",
        "  model.add(tf.keras.layers.Dense(32,use_bias=True,  activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(n,use_bias=False, activation='sigmoid'))\n",
        "  return model\n",
        "\n",
        "generator= generator()\n",
        "test = generator(x)\n",
        "print(test)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0.6038968  0.57279325 0.2797214  0.5034181  0.54567724 0.5274695\n",
            "  0.4871117  0.34068596]\n",
            " [0.5809688  0.5253811  0.31935957 0.54445416 0.52193296 0.5467386\n",
            "  0.5371016  0.3765967 ]\n",
            " [0.6787139  0.46849504 0.17689428 0.50378984 0.70877635 0.28082132\n",
            "  0.40639353 0.25080466]\n",
            " [0.5708337  0.61326265 0.2006315  0.39231458 0.5011916  0.49819288\n",
            "  0.65541124 0.3611753 ]\n",
            " [0.6107764  0.38870782 0.2262826  0.44968855 0.58791494 0.6424438\n",
            "  0.56435525 0.26370776]\n",
            " [0.6323948  0.48131266 0.27918065 0.5429769  0.5955866  0.40089893\n",
            "  0.37871546 0.4498343 ]\n",
            " [0.5289911  0.5787351  0.26225406 0.39141583 0.5112488  0.4935519\n",
            "  0.6350554  0.2984704 ]\n",
            " [0.52905047 0.45609656 0.3221706  0.51126    0.556043   0.5849875\n",
            "  0.64378524 0.36301965]\n",
            " [0.52541125 0.47984296 0.3409105  0.48033    0.4986676  0.5505903\n",
            "  0.54126483 0.3940817 ]\n",
            " [0.49718902 0.421244   0.34754276 0.5151671  0.5350225  0.5313148\n",
            "  0.561036   0.59233016]\n",
            " [0.6553266  0.42399383 0.20999476 0.5621243  0.484629   0.36619192\n",
            "  0.37282774 0.42651153]\n",
            " [0.5535167  0.51522756 0.29947    0.50165075 0.47993606 0.542394\n",
            "  0.6338056  0.6742907 ]\n",
            " [0.6475761  0.5696994  0.21824807 0.51405036 0.5447843  0.46928608\n",
            "  0.4559404  0.2050519 ]\n",
            " [0.59045243 0.51310617 0.25651637 0.46702948 0.54712355 0.57161397\n",
            "  0.64790714 0.3435889 ]\n",
            " [0.5753156  0.4670079  0.3381135  0.55243975 0.52969867 0.4522584\n",
            "  0.35923868 0.45167622]\n",
            " [0.47883445 0.54354364 0.37565252 0.46756905 0.45923918 0.5274447\n",
            "  0.52759784 0.53867453]\n",
            " [0.64387596 0.4866877  0.2993906  0.40266967 0.4352529  0.57266045\n",
            "  0.5097647  0.5441476 ]\n",
            " [0.5786179  0.60009426 0.34792987 0.49468443 0.48345703 0.56173086\n",
            "  0.53792566 0.63077015]\n",
            " [0.47268838 0.4636692  0.29826552 0.397299   0.5191203  0.56518716\n",
            "  0.6063538  0.39258254]\n",
            " [0.6635462  0.51070106 0.19196603 0.47829223 0.49206161 0.57873154\n",
            "  0.36481017 0.35944912]\n",
            " [0.56271785 0.57276446 0.2752285  0.44148842 0.52966267 0.43863857\n",
            "  0.60122466 0.46976158]\n",
            " [0.54085606 0.4598481  0.4142148  0.600496   0.50167876 0.4403585\n",
            "  0.60412425 0.35126626]\n",
            " [0.411459   0.45137548 0.48203    0.46805674 0.52735865 0.6747665\n",
            "  0.4823997  0.3878573 ]\n",
            " [0.5853345  0.47513977 0.26958105 0.37131065 0.5648008  0.55786043\n",
            "  0.6036813  0.31762153]\n",
            " [0.58422244 0.5008193  0.272098   0.5400139  0.591031   0.6348449\n",
            "  0.43348616 0.36620817]\n",
            " [0.632152   0.46579152 0.2500335  0.57333225 0.46838018 0.34563848\n",
            "  0.5574764  0.32612595]\n",
            " [0.512128   0.4903091  0.30566096 0.48558867 0.62162924 0.53225964\n",
            "  0.39280927 0.25910372]\n",
            " [0.5022833  0.5663566  0.36743954 0.4983496  0.44060296 0.4951411\n",
            "  0.47067836 0.38479015]\n",
            " [0.5991682  0.59411514 0.16946241 0.37548476 0.6650153  0.46760747\n",
            "  0.6699595  0.6339458 ]\n",
            " [0.54473245 0.49023154 0.3861067  0.42098826 0.54417896 0.5164514\n",
            "  0.5011572  0.44085738]\n",
            " [0.63289845 0.43585372 0.3457154  0.49877018 0.5159757  0.4629054\n",
            "  0.6160191  0.5718323 ]\n",
            " [0.5576054  0.5211772  0.38280824 0.4534178  0.4714562  0.4587853\n",
            "  0.39298797 0.46228606]\n",
            " [0.572002   0.52946746 0.17447522 0.60197973 0.45794994 0.62075627\n",
            "  0.3472017  0.32896572]\n",
            " [0.5958844  0.47564083 0.28446156 0.51200277 0.45064196 0.5582682\n",
            "  0.59702754 0.3382116 ]\n",
            " [0.50311494 0.52749604 0.2462135  0.40840322 0.4499754  0.68977475\n",
            "  0.8070327  0.40599585]\n",
            " [0.5321826  0.39955205 0.4421582  0.52186036 0.453834   0.6487156\n",
            "  0.5568809  0.46404073]\n",
            " [0.62941855 0.51348823 0.3301276  0.46448275 0.62790215 0.47138444\n",
            "  0.45357287 0.416591  ]\n",
            " [0.562124   0.55627507 0.3124386  0.48900047 0.5160723  0.44456604\n",
            "  0.50005287 0.45910215]\n",
            " [0.5613618  0.57954663 0.41608533 0.4916589  0.5228394  0.5466642\n",
            "  0.5308999  0.49015626]\n",
            " [0.42265385 0.42813724 0.3965676  0.54028744 0.5440837  0.5986135\n",
            "  0.4263169  0.5011101 ]\n",
            " [0.5364334  0.53550804 0.23485771 0.53715587 0.4799627  0.53682244\n",
            "  0.7752575  0.4885931 ]\n",
            " [0.5305374  0.5914643  0.37584835 0.4667168  0.4560192  0.56389016\n",
            "  0.4900992  0.44986838]\n",
            " [0.6054644  0.5283942  0.26095605 0.52211946 0.60034174 0.42430335\n",
            "  0.3595748  0.339818  ]\n",
            " [0.57648534 0.5219516  0.30795872 0.52743655 0.44974408 0.46851957\n",
            "  0.49508175 0.4276536 ]\n",
            " [0.67696494 0.5454164  0.23745733 0.5551176  0.57522494 0.5389583\n",
            "  0.5288705  0.6851693 ]\n",
            " [0.6628889  0.5562615  0.17167845 0.45787522 0.70759654 0.50221527\n",
            "  0.4396913  0.31802356]\n",
            " [0.6657633  0.4634319  0.32633105 0.48417065 0.5476833  0.65941215\n",
            "  0.6263671  0.5007261 ]\n",
            " [0.5505498  0.525731   0.2952556  0.510078   0.49181694 0.43859348\n",
            "  0.5116284  0.47312248]\n",
            " [0.6221844  0.6786787  0.23825431 0.3741783  0.5033361  0.5002786\n",
            "  0.4934311  0.28731996]\n",
            " [0.39194745 0.43082726 0.45609012 0.43851158 0.5726061  0.7669833\n",
            "  0.5172004  0.43042383]\n",
            " [0.48963466 0.59975064 0.18649495 0.346312   0.41320372 0.4615116\n",
            "  0.79860735 0.5833074 ]\n",
            " [0.5804194  0.49473447 0.31582367 0.52222013 0.46917558 0.47221884\n",
            "  0.49112839 0.3442484 ]\n",
            " [0.59756505 0.633424   0.30502295 0.45271394 0.47163057 0.57740426\n",
            "  0.53351665 0.31851834]\n",
            " [0.49801388 0.5313748  0.26922727 0.51206535 0.52330387 0.5292406\n",
            "  0.36302316 0.30455136]\n",
            " [0.53821784 0.46402517 0.30902433 0.47558886 0.47491753 0.643952\n",
            "  0.4177739  0.4944631 ]\n",
            " [0.6179551  0.5493187  0.27539065 0.42974904 0.5430258  0.5114136\n",
            "  0.5886236  0.2928305 ]\n",
            " [0.61489296 0.53079784 0.16500252 0.38883784 0.4382694  0.7741777\n",
            "  0.7328609  0.61142933]\n",
            " [0.5082986  0.48361856 0.42677307 0.46793872 0.46000695 0.6167696\n",
            "  0.45057878 0.46042418]\n",
            " [0.58603734 0.528779   0.29102603 0.43656915 0.5205003  0.44804248\n",
            "  0.5349409  0.37897393]\n",
            " [0.65936685 0.5007884  0.3124947  0.5758162  0.55863166 0.5389718\n",
            "  0.5781684  0.21356514]\n",
            " [0.54754806 0.6167819  0.23146579 0.4712179  0.42354965 0.51889324\n",
            "  0.39476356 0.3672896 ]\n",
            " [0.58238536 0.6003199  0.33973992 0.47171596 0.4786975  0.5872538\n",
            "  0.5856503  0.26767194]\n",
            " [0.5370644  0.5262141  0.2570492  0.43696848 0.54382354 0.53885996\n",
            "  0.5152975  0.3344754 ]\n",
            " [0.65100193 0.5504456  0.32406533 0.39433122 0.5638429  0.569004\n",
            "  0.49127936 0.30840027]\n",
            " [0.6123511  0.65845376 0.27850527 0.49859872 0.50467634 0.52283925\n",
            "  0.5506191  0.46684352]\n",
            " [0.4759897  0.5639626  0.3409636  0.34703246 0.5156578  0.5635704\n",
            "  0.5727021  0.48143852]\n",
            " [0.5702971  0.49463814 0.2888962  0.5013086  0.51037085 0.60349154\n",
            "  0.6112682  0.3261668 ]\n",
            " [0.44574225 0.35753065 0.50094754 0.5229976  0.46260148 0.7042942\n",
            "  0.54262114 0.458841  ]\n",
            " [0.4822971  0.53549606 0.2811462  0.43754303 0.6192409  0.5928578\n",
            "  0.3986573  0.30479902]\n",
            " [0.60335267 0.46348742 0.34659648 0.3847721  0.5714199  0.47589922\n",
            "  0.5364918  0.44322354]\n",
            " [0.47756666 0.5430652  0.302362   0.44265485 0.46830016 0.552612\n",
            "  0.51736456 0.331837  ]\n",
            " [0.5213719  0.4554172  0.32893252 0.46359497 0.50902313 0.4348472\n",
            "  0.4800535  0.40648553]\n",
            " [0.58969146 0.5331145  0.37936696 0.5489649  0.53731143 0.58185744\n",
            "  0.62691236 0.32538396]\n",
            " [0.5823771  0.5502334  0.28512788 0.42637235 0.5016582  0.5682137\n",
            "  0.6374988  0.4714829 ]\n",
            " [0.6083523  0.5048192  0.28596294 0.47844324 0.60315394 0.5022501\n",
            "  0.44957703 0.27687848]\n",
            " [0.5469399  0.43104756 0.36090016 0.52170235 0.5425303  0.46054476\n",
            "  0.38949317 0.36897838]\n",
            " [0.45665383 0.5380644  0.30033565 0.3596938  0.4734789  0.522411\n",
            "  0.63487697 0.3797741 ]\n",
            " [0.53490716 0.54921144 0.24544194 0.45645183 0.40036184 0.6672872\n",
            "  0.4227763  0.367397  ]\n",
            " [0.5537438  0.52291274 0.34451133 0.47621208 0.49013937 0.5292843\n",
            "  0.5999235  0.5484285 ]\n",
            " [0.53285265 0.49118575 0.23857474 0.5599211  0.3469087  0.4244834\n",
            "  0.7166922  0.41630888]\n",
            " [0.47031486 0.4917711  0.24639264 0.58471626 0.6525253  0.6051849\n",
            "  0.3426302  0.3401738 ]\n",
            " [0.49611104 0.59782976 0.3107183  0.37889645 0.39368606 0.61789924\n",
            "  0.6202218  0.58368224]\n",
            " [0.60153043 0.49072084 0.25705665 0.60224843 0.49172506 0.54729724\n",
            "  0.7097477  0.6191276 ]\n",
            " [0.5711451  0.5510504  0.31142464 0.48947698 0.46042165 0.4891377\n",
            "  0.6747068  0.4985953 ]\n",
            " [0.528159   0.53157663 0.34632656 0.3360255  0.55040616 0.6255723\n",
            "  0.54209125 0.44172674]\n",
            " [0.5787587  0.49755794 0.26986593 0.46577248 0.6171026  0.30285075\n",
            "  0.3773959  0.251842  ]\n",
            " [0.5119398  0.5360575  0.38017905 0.4422708  0.6177507  0.5947953\n",
            "  0.38001025 0.24626428]\n",
            " [0.5862291  0.63447887 0.23813504 0.3304804  0.50681525 0.50294304\n",
            "  0.5409344  0.48656633]\n",
            " [0.6064516  0.4912386  0.3105374  0.532691   0.51367795 0.4566605\n",
            "  0.596231   0.27730197]\n",
            " [0.5530971  0.41757008 0.35296175 0.5460113  0.43742535 0.5509887\n",
            "  0.5587119  0.4342702 ]\n",
            " [0.50447947 0.5700341  0.2754817  0.5691471  0.3592409  0.45074502\n",
            "  0.6148157  0.46486264]\n",
            " [0.4560899  0.5969977  0.36505815 0.38515043 0.48764053 0.56008136\n",
            "  0.46897653 0.38263604]\n",
            " [0.48890495 0.5099731  0.2565658  0.40865588 0.6616714  0.47163865\n",
            "  0.54850346 0.362693  ]\n",
            " [0.52605546 0.5119376  0.2429761  0.50563395 0.5508412  0.607915\n",
            "  0.66528285 0.41272405]\n",
            " [0.5897191  0.551926   0.27326947 0.40997332 0.5576546  0.5196475\n",
            "  0.5611624  0.32857728]\n",
            " [0.52481294 0.52697647 0.21017769 0.6339837  0.50681674 0.47851756\n",
            "  0.502669   0.48661873]\n",
            " [0.5377326  0.41038734 0.37763977 0.5987917  0.47530243 0.53512466\n",
            "  0.5702156  0.41071382]\n",
            " [0.55083907 0.5015102  0.37086666 0.49603665 0.5508657  0.50617725\n",
            "  0.5476142  0.4003218 ]\n",
            " [0.5161045  0.56303227 0.33292925 0.4275285  0.49375054 0.52448213\n",
            "  0.48659146 0.4519957 ]\n",
            " [0.65036666 0.46254167 0.2582457  0.45740455 0.51889026 0.49040374\n",
            "  0.46912652 0.36423552]], shape=(100, 8), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CbjziKpv35v",
        "colab_type": "text"
      },
      "source": [
        "### Help Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8rHD990Y-w8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV7pjryDv4M4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def EbNo2Sigma(ebnodb):\n",
        "    '''Convert Eb/No in dB to noise standard deviation'''\n",
        "    ebno = 10**(ebnodb/10)\n",
        "    return 1/np.sqrt(2*(2*k/n)*ebno)\n",
        "\n",
        "#numpy version of kl divergence\n",
        "def kl_divergence_np(p, q):\n",
        "    #use \"Laplace correction\" w to avoid zero and inf\n",
        "    w=1e-5\n",
        "    p = p + w\n",
        "    q = q + w\n",
        "    return np.sum(p * np.log(p / q))\n",
        "\n",
        "#tensorflow version of kl divergence\n",
        "def kl_divergence_tf(p, q):\n",
        "    #use \"Laplace correction\" w to avoid zero and inf\n",
        "    w = 1e-5\n",
        "    p = p + w\n",
        "    q = q + w\n",
        "    return tf.reduce_sum(p * tf.log(p / q))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EUzHiyUXLoP",
        "colab_type": "text"
      },
      "source": [
        "## Channels as Black-Box"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W63_fJJRXL7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_SNR_dB = 6\n",
        "noise_std = EbNo2Sigma(train_SNR_dB)\n",
        "\n",
        "def real_channel(x):\n",
        "    # Black-box Channel\n",
        "    #AWGN\n",
        "    return x + tf.random.normal(tf.shape(x), mean=0.0, stddev=noise_std)\n",
        "\n",
        "    #Rayleigh\n",
        "    #return x + tf.sqrt(tf.square(tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std)) + tf.square(tf.random_normal(tf.shape(x), mean=0.0, stddev=noise_std)))\n",
        "    \n",
        "    #Uniform U(-3;3)    \n",
        "    #return x + tf.random_uniform(tf.shape(x), minval=-2, maxval=2)\n",
        "\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzh-JZgfXSqN",
        "colab_type": "text"
      },
      "source": [
        "## Discriminator\n",
        "Model definition and creating discriminator\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97h2eMLeXS68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def concc(y,x):  \n",
        "  inputs = tf.concat(values=[y,x], axis=1)\n",
        "  return inputs\n",
        "\n",
        "def get_discriminator():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(32,use_bias=True, activation='relu',input_shape=((2*n,))))\n",
        "  model.add(tf.keras.layers.Dense(16,use_bias=True,  activation='relu'))\n",
        "  model.add(tf.keras.layers.Dense(1,use_bias=False,activation='sigmoid'))\n",
        "  return model\n",
        "discriminator = get_discriminator()\n",
        "\n",
        "#def discriminator(y,x):\n",
        "#    # Concatenate x and y\n",
        "#    inputs = tf.concat(values=[y,x], axis=1)\n",
        "#    #dense NN\n",
        "#    D_h1 = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)\n",
        "#    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
        "#    D_prob = tf.nn.sigmoid(D_logit)\n",
        "#    return D_prob, D_logit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRnlfRYuYC8R",
        "colab_type": "text"
      },
      "source": [
        "## Data Generation, überhaupt noch relevant??!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYcnkBIUXYa_",
        "colab_type": "text"
      },
      "source": [
        "## discriminator desicion????\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7im8FYMXeOV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-xQt6M5Xd9P",
        "colab_type": "text"
      },
      "source": [
        "## Define Loss\n",
        "strongly inspiered by: \\\\\n",
        "https://www.tensorflow.org/beta/tutorials/generative/dcgan?hl=en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36yIH7Q3FiEq",
        "colab_type": "text"
      },
      "source": [
        "## defining Loss. TODO:\n",
        "compile the Model with the right loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upCLjUsVDzAn",
        "colab_type": "code",
        "outputId": "2f689bb8-9f1c-43bb-8f22-a70a4f1002ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "x = tf.random.normal((batch_size,n),dtype=tf.dtypes.float32)    #randomly sample input data (\"fake\" AE messages)\n",
        "x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x)))\n",
        "real_training_data = tf.concat(values=[real_channel(x), x], axis=1)  \n",
        "fake_training_data = tf.concat(values=[generator(x, training =True),x], axis=-1)\n",
        "\n",
        "print(real_training_data.shape,fake_training_data.shape)\n",
        "real_output = discriminator(real_training_data)\n",
        "fake_output = discriminator(fake_training_data)\n",
        "print(fake_output)\n",
        "#print(real_output, fake_output)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 16) (100, 16)\n",
            "tf.Tensor(\n",
            "[[0.5108905 ]\n",
            " [0.5498549 ]\n",
            " [0.4180621 ]\n",
            " [0.51383394]\n",
            " [0.5040418 ]\n",
            " [0.4345404 ]\n",
            " [0.46084538]\n",
            " [0.4480617 ]\n",
            " [0.3934892 ]\n",
            " [0.47282144]\n",
            " [0.47103402]\n",
            " [0.49825177]\n",
            " [0.43301314]\n",
            " [0.51715964]\n",
            " [0.4375011 ]\n",
            " [0.55970156]\n",
            " [0.4904028 ]\n",
            " [0.49323368]\n",
            " [0.3945867 ]\n",
            " [0.53401923]\n",
            " [0.49759862]\n",
            " [0.47341555]\n",
            " [0.50998276]\n",
            " [0.4432954 ]\n",
            " [0.48498714]\n",
            " [0.44818753]\n",
            " [0.41238028]\n",
            " [0.44392666]\n",
            " [0.39482802]\n",
            " [0.38423488]\n",
            " [0.44208354]\n",
            " [0.47954473]\n",
            " [0.46887058]\n",
            " [0.403072  ]\n",
            " [0.5371058 ]\n",
            " [0.4836222 ]\n",
            " [0.4162647 ]\n",
            " [0.5572151 ]\n",
            " [0.3930797 ]\n",
            " [0.49137315]\n",
            " [0.5386938 ]\n",
            " [0.36822182]\n",
            " [0.56850404]\n",
            " [0.49524403]\n",
            " [0.5113517 ]\n",
            " [0.5939491 ]\n",
            " [0.49153143]\n",
            " [0.5203479 ]\n",
            " [0.4698678 ]\n",
            " [0.4864085 ]\n",
            " [0.50768054]\n",
            " [0.50494105]\n",
            " [0.5319094 ]\n",
            " [0.46770155]\n",
            " [0.5152778 ]\n",
            " [0.58456767]\n",
            " [0.47504413]\n",
            " [0.4937259 ]\n",
            " [0.5097818 ]\n",
            " [0.4628695 ]\n",
            " [0.48791876]\n",
            " [0.5635848 ]\n",
            " [0.51953757]\n",
            " [0.46947342]\n",
            " [0.5001008 ]\n",
            " [0.44311658]\n",
            " [0.4438137 ]\n",
            " [0.539209  ]\n",
            " [0.46086577]\n",
            " [0.41792777]\n",
            " [0.4686956 ]\n",
            " [0.47658038]\n",
            " [0.43126303]\n",
            " [0.5138752 ]\n",
            " [0.46966502]\n",
            " [0.48424456]\n",
            " [0.5758584 ]\n",
            " [0.38101956]\n",
            " [0.36146748]\n",
            " [0.4944349 ]\n",
            " [0.45569724]\n",
            " [0.49200788]\n",
            " [0.39270085]\n",
            " [0.46901256]\n",
            " [0.5134705 ]\n",
            " [0.46110737]\n",
            " [0.42484823]\n",
            " [0.4325941 ]\n",
            " [0.50975853]\n",
            " [0.6108914 ]\n",
            " [0.5542072 ]\n",
            " [0.53193545]\n",
            " [0.5272559 ]\n",
            " [0.6146739 ]\n",
            " [0.51105076]\n",
            " [0.40473914]\n",
            " [0.51423085]\n",
            " [0.47146   ]\n",
            " [0.4581721 ]\n",
            " [0.39376503]], shape=(100, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERelQ5oTEMtO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "  loss= -tf.reduce_mean(tf.math.log(real_output) + tf.math.log(1. - fake_output))   #use \"-\" sign to minimize rather than maximize loss\n",
        "  return loss\n",
        "  \n",
        "def generator_loss(fake_output, generator):\n",
        "  loss = -tf.reduce_mean(tf.math.log(fake_output))\n",
        "  return loss\n",
        "\n",
        "generator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "discriminator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gktABNcepz5c",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation with Histogram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgM9lv-dp1PI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_evaluation_data(batch_size=100):\n",
        "  x = tf.random.normal((batch_size,n),dtype=tf.dtypes.float32)    #randomly sample input data (\"fake\" AE messages)\n",
        "  x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x))) #Average power normalization (not required if standard normal distribution is used )\n",
        "  #G_n = tf.random.normal([tf.shape(x)[0],n],dtype=tf.float32) \n",
        "  #inputs = tf.concat(values=[x, G_n], axis=1)\n",
        "  fake_eval_data = tf.concat(values=[generator(x), x], axis=1)\n",
        "  real_eval_data = tf.concat(values=[real_channel(x), x], axis=1) #tf.concat(values=[real_channel(x),x], axis=1)\n",
        "  inputs = x\n",
        "  return  real_eval_data, fake_eval_data, inputs \n",
        "\n",
        "\n",
        "\n",
        "def get_evaluation_data(evaluation_per_epochs):\n",
        "  real_eval_data = []\n",
        "  fake_eval_data  = []\n",
        "  inputs = []\n",
        "  for i in range(evaluation_per_epochs):\n",
        "    data = generate_evaluation_data()\n",
        "    real_eval_data.append(data[0])\n",
        "    fake_eval_data.append(data[1])\n",
        "    inputs.append(data[2])\n",
        "  return real_eval_data, fake_eval_data, inputs\n",
        "\n",
        "\n",
        "def test_eval(real_eval_data,fake_eval_data,inputs):\n",
        "  hist_range = 2\n",
        "  \n",
        "  inputs_ = tf.concat(values=[inputs, inputs],  axis=-1)\n",
        "  \n",
        "  fake_output_hist = np.mean(fake_eval_data,axis=1)  # Changed from 0 to 1\n",
        "  real_output_hist = np.mean(real_eval_data,axis=1)\n",
        "  inputs_hist = np.mean(inputs_,axis=1)\n",
        "    \n",
        "  fake_output_hist1 = np.reshape( fake_output_hist,[-1,])\n",
        "  real_output_hist1 = np.reshape( real_output_hist,[-1,])\n",
        "    \n",
        "  plt.hist(fake_output_hist1,bins=100,range=(-hist_range,hist_range),density=True,histtype='step')\n",
        "  plt.hist(real_output_hist1,bins=100,range=(-hist_range,hist_range),density=True,histtype='step')    \n",
        "  plt.title(\"noise distribution\")\n",
        "  plt.legend([\"generator\", \"target\"])\n",
        "  plt.show()\n",
        "  \n",
        "  fake_noise = np.reshape( fake_output_hist - inputs_hist,[-1,])\n",
        "  real_noise = np.reshape( real_output_hist - inputs_hist,[-1,])\n",
        "   \n",
        "  plt.hist(fake_noise,bins=100,range=(-hist_range,hist_range),density=True,histtype='step')\n",
        "  plt.hist(real_noise,bins=100,range=(-hist_range,hist_range),density=True,histtype='step')    \n",
        "  plt.title(\"noise distribution after subtracting Inpus_noise\")\n",
        "  plt.legend([\"generator\", \"target\"])\n",
        "  plt.show()\n",
        "    \n",
        "    #print(\"decision for fake data was %d: and for real data was %d:\" % (decision_fake, decision_real))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXQWOgXnl62o",
        "colab_type": "text"
      },
      "source": [
        "### Define the training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sl75gEZl6Rv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 10\n",
        "steps_per_epoches = 50\n",
        "batch_size = 100\n",
        "\n",
        "evaluation_per_epochs = 100\n",
        "\n",
        "noise_dim = n        #noch ändern wenn ich noise ändere\n",
        "num_examples_to_generate = 16\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooDukkHvmduJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs, steps_per_epoches , batch_size):\n",
        "  start = time.time()\n",
        "  counter = 0\n",
        "  epoch = 0\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    #print(massege_batch)\n",
        "    counter += 1\n",
        "    train_step(epoch, steps_per_epoches , batch_size) \n",
        "    if counter%5 == 0:\n",
        "      print(\"counter %d:\" % (counter))\n",
        "    if counter%5 == 0:\n",
        "      real_eval_data, fake_eval_data, inputs = get_evaluation_data(evaluation_per_epochs)\n",
        "      test_eval(real_eval_data, fake_eval_data, inputs)\n",
        "    print ('Time for epoch {} is {} sec,'.format(epoch + 1, time.time()-start))\n",
        "    \n",
        "       \n",
        "        \n",
        "\n",
        "  print ('Time for the training is {} sec,'.format( time.time()-start))\n",
        "    \n",
        "   \n",
        "\n",
        "  # Generate after the final epoch\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7H98i7TmVxw",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XxSryMYmCkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@tf.function\n",
        "#def train_step(massege_batch,counter):\n",
        "#    x = tf.random.normal((batch_size,n),dtype=tf.dtypes.float32)    #randomly sample input data (\"fake\" AE messages)\n",
        "#    x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x)))\n",
        "    #G_n = tf.random.normal([tf.shape(x)[0],n],dtype=tf.float32) \n",
        "    #inputs = tf.concat(values=[x, G_n], axis=1)\n",
        "#    real_training_data = tf.concat(values=[real_channel(x), x], axis=1)  #tf.concat(values=[real_channel(x),x], axis=1)\n",
        "\n",
        "\n",
        " #   with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:           #tapes the gradient of the generaor an the discriminator\n",
        "  #    fake_training_data = tf.concat(values=[generator(x, training =True),x], axis=1)\n",
        "      \n",
        " #     real_output = discriminator(real_training_data, training=True)\n",
        " #     fake_output = discriminator(fake_training_data, training=True)\n",
        "\n",
        " #     disc_loss = -tf.reduce_mean(tf.math.log(real_output) + tf.math.log(1. - fake_output))   #use \"-\" sign to minimize rather than maximize loss\n",
        " #     gen_loss = -tf.reduce_mean(tf.math.log(fake_output))\n",
        "\n",
        " #     gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        " #     gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "  #    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "  #    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJno--QQh4_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "\n",
        "def train_step(epoch, steps_per_epoches , batch_size):\n",
        "\n",
        "    \n",
        "  for j in range(steps_per_epoches):\n",
        "    x = tf.random.normal((batch_size,n),dtype=tf.dtypes.float32)    #randomly sample input data (\"fake\" AE messages)\n",
        "    x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x)))\n",
        "    real_training_data = tf.concat(values=[real_channel(x), x], axis=1)\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      fake_training_data = tf.concat(values=[generator(x, training =True),x], axis=1)\n",
        "      real_output = discriminator(real_training_data, training=True)\n",
        "      fake_output = discriminator(fake_training_data, training=True)\n",
        "      disc_loss = -tf.reduce_mean(tf.math.log(real_output) + tf.math.log(1. - fake_output))   #use \"-\" sign to minimize rather than maximize loss\n",
        "      gen_loss = -tf.reduce_mean(tf.math.log(fake_output))\n",
        "      #print(disc_loss, gen_loss)\n",
        "          \n",
        "    \n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "      \n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuGMDjc1metC",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y82FQj3Jmvxx",
        "colab_type": "code",
        "outputId": "eb1a4b25-76db-4db7-f19a-d8e10b4f2bef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "train(epochs, steps_per_epoches , batch_size)\n",
        "\n",
        "generator.summary()\n",
        "discriminator.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time for epoch 1 is 37.39356255531311 sec,\n",
            "Time for epoch 2 is 23.254497051239014 sec,\n",
            "Time for epoch 3 is 22.960123538970947 sec,\n",
            "Time for epoch 4 is 23.53055691719055 sec,\n",
            "counter 5:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQDczSwkHuplZSvx/gs82EpcJ0wwAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0M3MMsIJ3cwsI5zQzcwywgndzCwj/hs7SPdqliM0cQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch 5 is 56.40851044654846 sec,\n",
            "Time for epoch 6 is 21.93162512779236 sec,\n",
            "Time for epoch 7 is 24.5101900100708 sec,\n",
            "Time for epoch 8 is 21.956520318984985 sec,\n",
            "Time for epoch 9 is 25.47157382965088 sec,\n",
            "counter 10:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAEICAYAAAB/Dx7IAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJXQRkZhQQhcRiYn/D/Mo/wnktDuHAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Time for epoch 10 is 54.93680763244629 sec,\n",
            "Time for the training is 54.93721342086792 sec,\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda (Lambda)              multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  544       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  1056      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  256       \n",
            "=================================================================\n",
            "Total params: 1,856\n",
            "Trainable params: 1,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1)                 16        \n",
            "=================================================================\n",
            "Total params: 1,088\n",
            "Trainable params: 1,088\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "CPU times: user 5min 9s, sys: 5.52 s, total: 5min 14s\n",
            "Wall time: 5min 12s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKRrixvwnA_m",
        "colab_type": "text"
      },
      "source": [
        "### Restore the latest checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sj-um_dZnD3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"training_1/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esovpz32jW6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#x = tf.random.normal((batch_size,n),dtype=tf.dtypes.float32)    #randomly sample input data (\"fake\" AE messages)\n",
        "#x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x)))\n",
        "#discriminator = keras.models.load_model('my_discriminator.h5')\n",
        "#generator = keras.models.load_model('my_generator.h5')\n",
        "#generator1 = generator(x)\n",
        "#generator1.fit(x, real_channel(x),  epochs = 10,\n",
        "#          validation_data = (x,real_channel(x)),\n",
        "#          callbacks = [cp_callback])\n",
        "\n",
        "#real_eval_data, fake_eval_data, inputs = get_evaluation_data(evaluation_per_epochs)\n",
        "#test_eval(real_eval_data, fake_eval_data, inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw6Rt5z3Rjud",
        "colab_type": "code",
        "outputId": "1a0a3e6d-4849-44bb-feb2-bab299d45ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "x = tf.random.normal((batch_size,n),dtype=tf.dtypes.float32)    #randomly sample input data (\"fake\" AE messages)\n",
        "x = x/tf.sqrt(2*tf.reduce_mean(tf.square(x)))\n",
        "#print(x)\n",
        "real_c = real_channel(x)\n",
        "fake_c = generator(x)\n",
        "\n",
        "print(fake_c-real_c)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[-0.26370656  1.1505326   1.4120816   0.8734156   0.6118694   0.22418636\n",
            "   1.3917325   1.0844246 ]\n",
            " [-0.48952827  0.87615085  0.47430754  0.27223182  0.5124152   0.62650883\n",
            "   2.094098    0.55299896]\n",
            " [ 0.36901242  0.1471864   0.5230961   0.7303144   1.0531926   0.31119573\n",
            "   1.4284958   0.9434923 ]\n",
            " [ 1.2287412   1.4152925  -0.29886281  0.15885752  3.108154    0.21572572\n",
            "   2.248306    0.8044444 ]\n",
            " [ 0.69195825 -0.27737963 -0.7777562   0.5089903   1.3578092   0.5460386\n",
            "   0.36087477  1.1469505 ]\n",
            " [ 1.9954338   1.1279942  -0.17386192 -0.27207303  0.6149676   2.1373134\n",
            "   0.2585684   1.4613531 ]\n",
            " [-0.4878283   0.6358883  -1.0117929   0.15016226  0.9645676   1.7035468\n",
            "  -0.17382455  0.814572  ]\n",
            " [ 0.52068734  0.8247038   0.5139618  -0.34613234  1.519618    1.9089639\n",
            "   1.1440014   1.6575595 ]\n",
            " [-0.32988742 -0.39902398 -0.58621806  1.2830262   0.87100416  0.08600495\n",
            "   0.6837716   1.1675167 ]\n",
            " [-0.19910944  0.3253397   0.13065082 -2.0486782  -0.45329332 -0.27421057\n",
            "  -0.5149729   1.4993087 ]\n",
            " [-0.6787706   0.74579555  1.4837029  -0.7670139  -0.13515472 -0.9232954\n",
            "  -0.35190558  0.89978915]\n",
            " [ 1.0755045   0.6916744  -0.22954345  0.07509903  2.7292082   0.50001734\n",
            "   2.3662636   1.1486678 ]\n",
            " [-0.47053847  1.0998778   0.07434572  0.24850643  1.884224    0.8331828\n",
            "  -0.3455696   0.9935695 ]\n",
            " [ 0.16196021 -0.126852   -0.1962856  -0.29677495  0.6413221  -0.2633302\n",
            "   0.4634531   0.5695712 ]\n",
            " [ 1.8814855   0.5478601   1.0909463  -0.43622163  0.2626392   1.3641224\n",
            "  -0.15547991  0.00521654]\n",
            " [ 2.3282719   1.1326318   0.36355266 -0.5936474   1.3501403   1.3315067\n",
            "   0.09153795  0.9043721 ]\n",
            " [ 0.9308446  -0.21447945  0.3160879   1.9984648   0.75185174  0.93749845\n",
            "   0.6756315   0.0916537 ]\n",
            " [-1.3624132   0.5214204  -0.26864904 -0.31517732  1.2986389   0.32591453\n",
            "   0.8253762  -0.41795325]\n",
            " [-1.3109024   1.1578969  -0.06380653  0.09163576  1.365721    0.6373145\n",
            "   0.6456313  -0.26843226]\n",
            " [ 0.5249475   1.1759558  -0.15182592 -0.41364962  1.4879068   1.0148233\n",
            "   1.4167454   1.6666858 ]\n",
            " [ 1.0348709   0.4970206   0.03760493 -0.73641217  1.6691954   0.11946982\n",
            "   0.90660316  0.17153424]\n",
            " [-0.75886166  0.07718638  0.97602767  0.24550901 -0.2194128  -0.22497715\n",
            "   0.2558384   0.13703686]\n",
            " [ 1.8274479   0.8571814   1.5088077  -0.14094858  0.88220054  2.3456068\n",
            "   2.2548676  -0.21757352]\n",
            " [-1.0077239   0.18692285  0.9212607  -0.11359599 -0.55562353  1.1760279\n",
            "   0.44242668 -0.3352424 ]\n",
            " [ 1.0152403  -1.0921018  -1.8146915   2.6482062   1.6326046   1.1941396\n",
            "   1.1432364  -0.07534903]\n",
            " [ 0.05367914  1.0507884   0.11284655  1.2454298   2.132532    1.6272194\n",
            "   0.8510119   1.69567   ]\n",
            " [ 1.3347099  -0.0099411   0.1887852   0.4166373   1.5680535  -0.36447787\n",
            "   1.7162437  -0.01558113]\n",
            " [ 0.57300174  0.24929856 -0.39833438  1.0143276   1.6111946  -0.2599013\n",
            "   0.21476328  2.2401881 ]\n",
            " [ 2.0375633  -0.25249767 -0.07608312 -0.00702508 -0.20354262  1.5355649\n",
            "   1.2392802  -0.13962692]\n",
            " [ 0.8544698   1.3641449  -0.64748675 -0.07169452  0.25786674  1.4684584\n",
            "   0.57135653  0.01693678]\n",
            " [ 1.6418165   1.7519157   1.1076232   0.07448309  0.79348785  1.1372262\n",
            "   1.1127543  -0.66026926]\n",
            " [ 0.93646014  0.93835104 -0.36023593  0.7812262   1.15149     0.36108196\n",
            "   0.69463307  0.09640157]\n",
            " [-0.50058377 -0.6711028  -0.11512351  0.54176867  1.1576384  -1.6498821\n",
            "   0.0463115   1.2144467 ]\n",
            " [ 1.7647104  -0.07725734  0.02948496  0.6242739   0.504827    0.39806724\n",
            "  -0.1583035   1.686677  ]\n",
            " [-0.11863395  0.79795414 -0.23502463  0.7623573   0.4051029   0.4461709\n",
            "   1.187587    0.63306355]\n",
            " [ 1.4901149   1.6706831   1.9627087   0.95849127  0.44785976  1.1197705\n",
            "   0.9413559  -0.5486424 ]\n",
            " [-1.274333    0.81451726  0.99129766  0.9908852  -0.4649801  -0.01042792\n",
            "   0.5652162   0.15672833]\n",
            " [ 0.5399636   0.23254526  2.0135307  -1.0430281  -0.3119911   0.3681273\n",
            "   2.0470014   0.5317414 ]\n",
            " [-0.02322692 -0.1487607   1.199318    1.8136753   0.77197814  0.39323154\n",
            "   0.82713044 -0.20781112]\n",
            " [ 0.6135891   1.7114968   1.289724    1.1570057   0.6181731   1.9189144\n",
            "   1.3033429   2.1553416 ]\n",
            " [ 0.87020946  0.9751724   0.5545516   0.6393546   0.14815378  0.93353236\n",
            "  -0.16765335  0.4201247 ]\n",
            " [ 0.21293119  0.58523184  1.4297628   0.5398477   0.37886852  0.09249276\n",
            "   0.54465187  1.4970689 ]\n",
            " [ 1.4854     -0.23486507  0.22184193 -0.4284293  -0.8462894   1.9342464\n",
            "   1.0360724   0.6442965 ]\n",
            " [ 1.8778315   1.3601345   0.6798148   0.32814136  1.3316138   0.06170613\n",
            "  -0.0282566  -1.144913  ]\n",
            " [-1.252023   -0.15474424 -0.8492366   0.3007896   0.0687777   0.04891036\n",
            "   0.23120809  0.29058897]\n",
            " [ 0.15183681  0.47186205 -0.2320419   0.6167193  -0.20796499  0.6844659\n",
            "   0.8485405   1.5739775 ]\n",
            " [-1.0486115  -0.5698384   1.5620301   0.33295217  0.2572677   0.1864757\n",
            "   0.74972874  0.7522066 ]\n",
            " [ 2.114574    0.9088516  -0.48206246 -0.0899959   0.18464392  1.3949122\n",
            "   0.6988815   0.8555602 ]\n",
            " [-0.09539952  0.8823197   1.1780024  -1.5048363   0.4265647   1.4471792\n",
            "   1.0342188   2.2055712 ]\n",
            " [ 0.91664314  0.83806086 -0.03357874 -1.09203     0.9828151   2.0582886\n",
            "   1.414376    0.37523788]\n",
            " [-0.87867475  0.71485555  1.0288854  -0.4381567   1.5285811  -0.01447031\n",
            "   1.1974354   0.30196702]\n",
            " [ 0.05979948  1.0124137   0.03755346 -0.22224009  0.33704868  1.2175872\n",
            "   1.3474967   0.93961936]\n",
            " [ 1.7526009   0.44951397 -0.10173929  0.20309643 -0.5326115   0.44408607\n",
            "  -0.7158653   0.519681  ]\n",
            " [ 0.11348201  0.755297    0.41244853 -0.20083752 -0.18611503 -1.0287936\n",
            "  -0.09152619  1.0578461 ]\n",
            " [ 1.7896836   0.8152348   2.420651    0.30087292  0.2838658   1.454196\n",
            "   1.8756275   0.83321315]\n",
            " [ 0.55647326  2.1323292   0.45141524 -1.5318285   0.7432907   0.7237375\n",
            "   1.5342665   1.1674173 ]\n",
            " [ 0.22326678  1.103112    1.1428342  -0.71124184  0.6117129   0.6120769\n",
            "   0.5601216   1.0218421 ]\n",
            " [ 2.2304738   0.36539406  0.01335412  1.6477116  -0.48797023  1.784745\n",
            "   0.35958654 -0.86860234]\n",
            " [-0.19405887  0.38956925  0.25162816  0.01160306  0.6550411   0.425375\n",
            "   0.42233825  1.3800781 ]\n",
            " [-0.15249273  0.2828344  -0.29038578  0.8499003   0.26554793 -0.4748422\n",
            "  -0.18263805  0.09610927]\n",
            " [-0.4505646   0.20626232  0.4385914   1.6373683   0.7081803   0.4125571\n",
            "   0.9032756   0.35702264]\n",
            " [ 0.42473713  1.0904115   1.338297    2.2517388   0.09096026  0.8952932\n",
            "   0.09266818  0.5473115 ]\n",
            " [-1.6278032   0.12930429  0.39340705 -0.41350546  0.06978929 -0.2684794\n",
            "   0.83155054  1.8062251 ]\n",
            " [-0.85070395  1.0825223   1.651455   -0.24562486 -0.9115027   1.0285721\n",
            "   0.78094095  0.22743511]\n",
            " [-0.722713    0.6268024   1.8071129   1.5448797  -0.85480165 -0.14109628\n",
            "  -0.14718273  1.1689187 ]\n",
            " [-0.20255244 -0.1274637   1.1672972   1.185318    0.81896377  2.228133\n",
            "   1.6905608   0.41921037]\n",
            " [-1.23046     1.2939938   1.56431     0.33314103  1.9067014  -1.4635708\n",
            "   1.1367439   2.9524496 ]\n",
            " [ 1.6154494   0.89371973  2.035874   -0.56574064 -0.5296787   1.71156\n",
            "   1.0754043   0.60696   ]\n",
            " [ 1.1891632   1.0281132   0.96484613  2.3004997  -0.051347    0.8782215\n",
            "   0.20237905  0.47498128]\n",
            " [ 0.635803    0.9374471   0.33291686 -0.5971026   1.3738196   0.67796195\n",
            "   1.1700046   1.3495141 ]\n",
            " [-0.5548004  -0.20027238 -1.1056142  -1.5782385   0.08719653  0.95917886\n",
            "   0.37600768  0.1767866 ]\n",
            " [ 1.176583    1.0462544   0.211208   -0.29597205  0.10848719  0.3691145\n",
            "  -0.34155333  1.0405554 ]\n",
            " [ 0.5923722   0.4446863   0.88366604 -0.29006782  1.249502   -0.5672513\n",
            "  -0.6096859   1.5071554 ]\n",
            " [ 1.2542175  -0.7185159   0.73199034 -0.07916397 -0.24340695  0.60566545\n",
            "   0.09483278 -0.5757801 ]\n",
            " [ 0.34488702  1.0772052   1.9029405   0.2373712   0.28038073  2.5756602\n",
            "   0.9746988   1.8201318 ]\n",
            " [ 0.5990418   1.6420207   2.0923362   0.23398063  0.8112044   0.7732497\n",
            "   1.0263824   2.2016506 ]\n",
            " [-0.9706243   0.11965233  0.6485505   0.02598006 -0.27198637  1.2303905\n",
            "   1.079628    1.7946099 ]\n",
            " [ 0.05363292  0.7046287  -0.97193265  1.2571747   1.0352852   0.5111383\n",
            "   0.5457063   0.65591997]\n",
            " [ 1.674681    1.1061008   1.1626805  -0.14816871  0.9230962  -0.40034747\n",
            "   1.8137997   0.21149746]\n",
            " [-0.7477492  -0.77030146 -0.53481966 -0.5365444   0.6207249  -0.61964405\n",
            "  -0.730134    1.4863861 ]\n",
            " [-1.52216     0.32213593  1.3916289  -1.2520926   0.6693661  -0.42350668\n",
            "   0.8708966   1.196865  ]\n",
            " [-0.05948433  0.17106056 -0.09701729  0.16712779  0.7160822   0.68163764\n",
            "   0.43821245  0.13929605]\n",
            " [-0.47217304  0.54430294 -0.661402   -0.31817722  1.7801012   0.5899374\n",
            "   1.4556569   0.24757266]\n",
            " [ 0.47332078 -0.21614899 -0.16203612  0.43668813 -0.6598714  -1.0536588\n",
            "  -0.25885856  0.8569042 ]\n",
            " [-0.59117085  0.45383888  0.28064454  0.04989405 -0.6752417  -0.56492245\n",
            "   0.0548583   0.02326733]\n",
            " [-0.63895994  0.2678026   0.18470824 -0.32188842  1.4978591  -0.54025304\n",
            "   1.0911812   1.6399634 ]\n",
            " [ 1.6686405   0.7207497   1.0340672  -0.564002    0.5969939   0.22923869\n",
            "   1.2576987   0.17242032]\n",
            " [ 0.74866164 -0.4521135   0.7837838   0.10968052  0.5751214  -0.0471437\n",
            "   0.353351    0.45419154]\n",
            " [ 1.1160336   1.184033    2.16509    -0.2538356   0.18025422  0.98231554\n",
            "   1.2369692  -0.69969845]\n",
            " [-1.7480419  -0.11696586  0.5591637   0.04825804 -0.22445287 -0.63932073\n",
            "   0.3434801   1.5293486 ]\n",
            " [ 1.4472158  -0.23057985  0.36098325 -0.6163057   0.50312924  1.4224143\n",
            "  -0.5884464   1.1138387 ]\n",
            " [-0.06744031  0.74858046  0.48484772  0.5405695   1.0655653   1.5743543\n",
            "   0.7900896   0.47174245]\n",
            " [ 0.8381337   1.0883762   0.4417408  -0.4691053   1.9279872   0.97116584\n",
            "   0.99200976  0.9502444 ]\n",
            " [-1.1718663   0.61986375  2.2137167  -0.2524112   1.2046716   0.43114156\n",
            "   0.68681276  1.378402  ]\n",
            " [-0.95663387  0.96943015  0.03516614  1.0397035   0.78689945 -0.17914575\n",
            "   1.0581595   1.2554718 ]\n",
            " [-0.80676806  1.2890971   0.62070215  0.14650537  0.60752237 -0.13468057\n",
            "   1.2316968   3.0652313 ]\n",
            " [ 0.64268696  0.5699378  -0.5614658   0.29608223  1.0334982  -0.34006208\n",
            "   0.5373843   1.6274993 ]\n",
            " [ 1.5112869   1.4192576   1.1818154  -0.42258185  0.874302    1.9250724\n",
            "   1.1615106   0.5980308 ]\n",
            " [-0.00638759  0.50827754  1.1880383  -1.4232228  -0.47955996  0.7760951\n",
            "   0.76315504  0.6115426 ]\n",
            " [ 1.0195131   0.11473501  0.66272914  0.6742512   1.4562228   0.56166965\n",
            "   1.213845    0.6015869 ]], shape=(100, 8), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmP50TkiAg-C",
        "colab_type": "text"
      },
      "source": [
        "## AE\n",
        "Die Idee sollte sein das Training auf den encoder und decoder einzuschränken. Jedoch soll **end-to-end** trainiert werden, hierfür sollte vllt eine art Funktion eingesetzt werden, welche über die GAN's Layer zurück geht.\n",
        "Muss ich hierfür die Layer nochmals einzeln definieren?\n",
        "\n",
        "\n",
        "***Vermutung: Der Ausgang hat die 8fache dimension des Eingangs-> daher nur 1/8 richtig oder 7/8 richtig*** \\\\\n",
        "**zu klären: was passiert in meinem AE dass sie dei dimension ver8-facht von (1000,8) zu (8000,n)**\n",
        "**Kontrollieren was der output von meinem GAN ist**\n",
        "**Add complexity for higher rubustness**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiuN3SZYpeTU",
        "colab_type": "code",
        "outputId": "1c99a419-ea3b-4f9d-e475-8c52779cb796",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 892
        }
      },
      "source": [
        "\n",
        "\n",
        "def get_encoder():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(32,use_bias=True, activation='relu',input_shape=(n,)))\n",
        "  model.add(tf.keras.layers.Dense(n,use_bias=False, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "def get_decoder():\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(n,use_bias=True, activation='relu', input_shape=(n,)))\n",
        "  model.add(tf.keras.layers.Dense(n,use_bias=False, activation='softmax'))\n",
        "  return model\n",
        "\n",
        "encoder = get_encoder()\n",
        "decoder = get_decoder()\n",
        "\n",
        "encoder.summary()\n",
        "generator.summary()\n",
        "decoder.summary()\n",
        "   \n",
        "def get_AE(encoder, generator, decoder):\n",
        "  AE_model = tf.keras.Sequential()\n",
        "  AE_model.add(encoder)\n",
        "  AE_model.add(tf.keras.layers.Lambda(generator))\n",
        "  AE_model.add(decoder)\n",
        "  return AE_model\n",
        "          \n",
        "    \n",
        "def generate_data_vector(length):\n",
        "  random_vector = tf.random.uniform(shape =(length,),minval=0,maxval=n-1, dtype=tf.dtypes.int32 ,seed=None,name=None)\n",
        "  random_hot_one_vector = tf.one_hot(random_vector, depth=n,on_value=1, off_value=0,axis=-1)\n",
        "  print(random_hot_one_vector.shape)\n",
        "  return random_hot_one_vector\n",
        "\n",
        "data, test_data = generate_data_vector(1000000), generate_data_vector(10000)\n",
        "#print(data)\n",
        "\n",
        "#model = Autoencoder()\n",
        "AE = get_AE(encoder, generator, decoder)\n",
        "AE.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "history = AE.fit(data, data, batch_size=100,steps_per_epoch=1100, epochs=5)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 32)                288       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 8)                 256       \n",
            "=================================================================\n",
            "Total params: 544\n",
            "Trainable params: 544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lambda (Lambda)              (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 32)                544       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 8)                 256       \n",
            "=================================================================\n",
            "Total params: 1,856\n",
            "Trainable params: 1,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 8)                 64        \n",
            "=================================================================\n",
            "Total params: 136\n",
            "Trainable params: 136\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(1000000, 8)\n",
            "(10000, 8)\n",
            "Train on 1000000 samples\n",
            "Epoch 1/5\n",
            " 107200/1000000 [==>...........................] - ETA: 21s - loss: 1.1906 - accuracy: 0.5377Epoch 2/5\n",
            " 109000/1000000 [==>...........................] - ETA: 16s - loss: 0.1196 - accuracy: 0.9981Epoch 3/5\n",
            " 109500/1000000 [==>...........................] - ETA: 16s - loss: 0.0134 - accuracy: 1.0000Epoch 4/5\n",
            " 107900/1000000 [==>...........................] - ETA: 16s - loss: 0.0045 - accuracy: 1.0000Epoch 5/5\n",
            " 107400/1000000 [==>...........................] - ETA: 16s - loss: 0.0018 - accuracy: 1.0000"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-ZsnSNgM7g2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_SNR_dB = 6\n",
        "\n",
        "def analytic_channel(input): \n",
        "  #print(input.shape)\n",
        "  return input + tf.random.normal(tf.shape(input), mean=0.0, stddev=noise_std)\n",
        "\n",
        "def real_transmision(test_data):\n",
        "  y = encoder(test_data)\n",
        "  y = generator(y)\n",
        "  y = decoder(y)\n",
        "  return y\n",
        "  #model = tf.keras.Sequential()\n",
        "  #model.add(encoder)\n",
        "  #model.add(tf.keras.layers.Lambda(generator))\n",
        "  #model.add(tf.keras.layers.Lambda(real_channel))\n",
        "  #model.add(decoder)\n",
        "  #return model\n",
        "\n",
        "def test_diff_eval(test_data, results):\n",
        "  diff = []\n",
        "  for i in range(tf.shape(test_data)[0]):\n",
        "    diff.append(tf.math.subtract(test_data[i,:], results[i,:]))\n",
        "  return diff\n",
        "    \n",
        "  \n",
        "real_AE = real_transmision(test_data)\n",
        "testTest = tf.dtypes.cast(real_AE + tf.constant(0.1,dtype=tf.float32,shape=tf.shape(real_AE)), tf.int32)\n",
        "\n",
        "diff_test =  test_diff_eval(test_data, testTest) \n",
        "#t = tf.math.subtract(test_data[1,:], real_AE[1,:])\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SntX-i_2J76v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ea5e4d4-87b8-41e5-b343-41a83e59a9af"
      },
      "source": [
        "print(sum(diff_test))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([0 0 0 0 0 0 0 0], shape=(8,), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDfTMdthneHM",
        "colab_type": "text"
      },
      "source": [
        "## Trainingparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIQ1bKE_nJSq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_EbNodB = 6\n",
        "val_EbNodB = train_EbNodB\n",
        "\n",
        "training_params = [\n",
        "    #batch_size, lr, ebnodb, iterations\n",
        "    [100    , 0.001, train_EbNodB, 1000],\n",
        "    [100    , 0.0001, train_EbNodB, 10000],\n",
        "    [1000    , 0.0001, train_EbNodB, 10000]\n",
        "]\n",
        "\n",
        "validation_params = [\n",
        "    #batch_size, ebnodb, val_steps \n",
        "    [100000, val_EbNodB, 100],\n",
        "    [100000, val_EbNodB, 1000],\n",
        "    [100000, val_EbNodB, 1000]\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SR4RrE3nqTc",
        "colab_type": "text"
      },
      "source": [
        "## Create and train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLzQO7yQnP1p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "2deab129-aab4-4eaf-cf32-238795ef9c22"
      },
      "source": [
        "model_file_baseline = 'models/ae_baseline_k_{}_n_{}'.format(k,n)\n",
        "\n",
        "ae_baseline = AE(k,n,useGAN=False,seed=seed)\n",
        "ae_baseline.train(training_params, validation_params)\n",
        "\n",
        "ae_baseline.save(model_file_baseline)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a3cfbfeb8464>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_file_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'models/ae_baseline_k_{}_n_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mae_baseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0museGAN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mae_baseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    850\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 851\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: call() got an unexpected keyword argument 'useGAN'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi_IcVrbnS1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}